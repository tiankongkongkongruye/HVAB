## 一致性结论
- 进阶步骤（窗口中心级稀疏全局注意力、全局token上限、按开关跳过全局路径）属于工程化的“计算优化”，保持论文的三大组件与损失不变：CM‑FSC、GLC‑Former（门控融合）、DEN‑Phy 与最终重建逻辑。
- 它们不改论文的目标函数与输出路径，只将全局注意力的计算从 O(N²) 降到可承受的规模；可视为对“全局关系建模”的近似实现。

## 与论文可能的差异边界
- 差异仅在“全局注意力的数值实现”层面：
  - 论文理想是全 token 的全局自注意力；优化版改为“窗口中心级全局”或“下采样后全局”，是稀疏/低频近似。
  - 门控融合、局部路径、逆小波重建、损失项均保持一致。
- 为避免任何方法学争议，提供“严格模式”保留论文版实现：在资源允许时仍可启用完整全局注意力。

## 实施策略
- 增加运行开关：
  - `--glc_global_mode {exact, center, pooled}`：
    - `exact`：严格论文版全 token 注意力（可能 OOM）；
    - `center`：窗口中心级全局（推荐，降 OOM）；
    - `pooled`：超限时对序列池化到 `--global_tokens_limit` 再做全局。
  - `--global_tokens_limit`：默认 8192，控制最大全局token数。
- 保持损失与输出：`L_freq/L_cons/L_phy/L_recon` 与 `full/hybrid` 输出路径不变。
- 文档声明：优化模式为“计算近似”，不改变方法学；论文复现实验时用 `exact` 与较小 `crop_size`。

## 使用建议
- 论文严格复现：`--glc_global_mode exact --crop_size 128 --lambda_C 0.1`（或更小裁剪避免 OOM）。
- 训练高效稳定：`--glc_global_mode center --global_tokens_limit 8192 --crop_size 256 --lambda_C 0.1`。

## 交付
- 我将按上述策略加入可选开关，默认使用优化模式，提供“严格模式”以确保与论文完全一致；并给出两套命令以复现实验与高效训练。